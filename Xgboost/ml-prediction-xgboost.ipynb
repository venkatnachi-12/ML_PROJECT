{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116098,"databundleVersionId":14140597,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":598.317571,"end_time":"2025-10-22T16:38:42.166043","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-22T16:28:43.848472","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"eb2584ec","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-26T17:22:52.817804Z","iopub.execute_input":"2025-10-26T17:22:52.818019Z","iopub.status.idle":"2025-10-26T17:22:54.758079Z","shell.execute_reply.started":"2025-10-26T17:22:52.817997Z","shell.execute_reply":"2025-10-26T17:22:54.757068Z"},"papermill":{"duration":1.823329,"end_time":"2025-10-22T16:28:49.982133","exception":false,"start_time":"2025-10-22T16:28:48.158804","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/sample_submission.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\n","output_type":"stream"}],"execution_count":1},{"id":"54668191","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor \nfrom sklearn.metrics import mean_squared_error\n\n\n\nCLIP_FLOOR = 1.0        #Transpost costs below 1 are clipped to 1\nFREQ_THRESHOLD = 0.01   # Used to find rare categories\nLEARNING_RATE = 0.02    # Slightly higher than 0.01 to speed up convergence\nN_ESTIMATORS = 1500     # Increased number of trees for lower bias\n\n\ntrain_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\")\n\ntest_ids = test_df['Hospital_Id']\ntrain_df.set_index('Hospital_Id', inplace=True)\ntest_df.set_index('Hospital_Id', inplace=True)\n\n#Defining X_train ,y_train,X_test so that model can be trained on training data and tested on X_test \ny_train=train_df['Transport_Cost'].copy()\nX_train = train_df.drop(columns=[\"Transport_Cost\"]).copy()\nX_test = test_df.copy()\n\n#Combining training and testing dataframes for preprocessing\ncombined_df = pd.concat([X_train, X_test], axis=0)\n\n#Log transformation on clipped transport costs\ny_train_transformed = np.log(y_train.clip(lower=CLIP_FLOOR))\n\n#Dropping unnecesary columns and adding new columns with important information and doing some formatting\ncombined_df['Order_Placed_Date'] = pd.to_datetime(combined_df['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Date'] = pd.to_datetime(combined_df['Delivery_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Lag_Days'] = (combined_df['Delivery_Date'] - combined_df['Order_Placed_Date']).dt.days.fillna(0).astype(int)\ncombined_df['Order_Day_of_Week'] = combined_df['Order_Placed_Date'].dt.dayofweek\ncombined_df['Order_Month'] = combined_df['Order_Placed_Date'].dt.month\n\n# Equipment related new features\ncombined_df['Equipment_Volume'] = combined_df['Equipment_Height'] * combined_df['Equipment_Width']\ncombined_df['Equipment_Density'] = combined_df['Equipment_Weight'] / (combined_df['Equipment_Volume'] + 1e-6) #1e-6 added to handle 0 volume\n\n# Drop original dates and low-utility features\ncombined_df.drop(columns=['Order_Placed_Date', 'Delivery_Date', 'Supplier_Name', 'Hospital_Location'], inplace=True, errors='ignore')\n\n#For binary columns map to 1 or 0 according to binary_map\nbinary_map = {'Yes': 1, 'No': 0}\nbinary_cols_to_map = ['CrossBorder_Shipping', 'Installation_Service', 'Rural_Hospital'] \nfor col in binary_cols_to_map:\n    if col in combined_df.columns:\n        combined_df[col] = combined_df[col].map(binary_map).fillna(0)\n    \nbinary_cols = binary_cols_to_map + ['Urgent_Shipping', 'Fragile_Equipment'] \n\n#  Group low frequency categorical features\ncategorical_cols_to_group = ['Equipment_Type', 'Transport_Method', 'Hospital_Info']\nfor col in categorical_cols_to_group:\n    if col in combined_df.columns:\n        train_counts = combined_df.iloc[:len(X_train)][col].value_counts(normalize=True)\n        low_freq_cats = train_counts[train_counts < FREQ_THRESHOLD].index\n        combined_df[col] = np.where(combined_df[col].isin(low_freq_cats), 'Other', combined_df[col])\n\n\n# Re-separate the data after doing some preprocessing on both training and test data\nX_train_clean = combined_df.iloc[:len(X_train)]\nX_test_clean = combined_df.iloc[len(X_train):]\n\n\n\n#Find numerical and categorical columns\nnumeric_cols = [col for col in X_train_clean.select_dtypes(include=np.number).columns.tolist() if col not in binary_cols]\ncategorical_cols = X_train_clean.select_dtypes(include=['object']).columns.tolist()\n#Preprocessor pipeline\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ],\n    remainder='passthrough'\n)\n\n\n\n\n# Define the xgboost base model\nxgb_base = XGBRegressor(\n    objective='reg:squarederror', \n    n_estimators=N_ESTIMATORS, \n    learning_rate=LEARNING_RATE,\n    random_state=42, \n    n_jobs=-1\n)\n\ntuning_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', xgb_base)\n])\n\n#Parameter grid used for searching by GridSearchCV\nparam_grid = {\n    'regressor__max_depth': [6, 8, 10],            \n    'regressor__subsample': [0.75, 0.9],           \n    'regressor__colsample_bytree': [0.7, 0.9],     \n    'regressor__reg_alpha': [0.001, 0.1],          # L1 regularization \n    'regressor__reg_lambda': [0.1, 1]              # L2 regularization \n}\ncv = KFold(n_splits=4, shuffle=True, random_state=42)\n\ngrid_search = GridSearchCV(\n    tuning_pipeline, \n    param_grid, \n    cv=cv, \n    scoring='neg_root_mean_squared_error',\n    verbose=1, \n    n_jobs=-1\n)\n\ngrid_search.fit(X_train_clean, y_train_transformed)\nbest_params_found = grid_search.best_params_\nbest_score = -grid_search.best_score_\nprint(f\"Best Cross-Validation RMSE (on log-target): {best_score:.4f}\")\n\n# Clean up parameter names and add fixed values for the final model\nfinal_xgb_params = {k.replace('regressor__', ''): v for k, v in best_params_found.items()}\nfinal_xgb_params['n_estimators'] = N_ESTIMATORS\nfinal_xgb_params['learning_rate'] = LEARNING_RATE\nfinal_xgb_params['objective'] = 'reg:squarederror'\nfinal_xgb_params['random_state'] = 42 \nfinal_xgb_params['n_jobs'] = -1\n\nprint(\"Best XGBoost parameters for final training:\")\nprint(final_xgb_params)\n\n\n\n\n\n\n# Initialize the xgboost optimised model\nxgb_best_model = XGBRegressor(**final_xgb_params)\n\n# Define final pipeline\nfinal_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', xgb_best_model)\n])\n\n# Fit the final pipeline on all training data\nfinal_pipeline.fit(X_train_clean, y_train_transformed)\n\n# Make predictions\npredictions_log = final_pipeline.predict(X_test_clean)\n\n# Inverse Transform Predictions (e^x)\npredictions_original_scale = np.exp(predictions_log) \npredictions_original_scale = np.maximum(predictions_original_scale, 0)\n\n#Submission dataframe\nsubmission_df = pd.DataFrame({\n    'Hospital_Id': test_ids,\n    'Transport_Cost': predictions_original_scale\n})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nprint()\nprint(\"Submission file 'submission.csv' successfully created.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-10-26T17:22:54.759823Z","iopub.execute_input":"2025-10-26T17:22:54.760191Z","iopub.status.idle":"2025-10-26T17:33:00.407410Z","shell.execute_reply.started":"2025-10-26T17:22:54.760171Z","shell.execute_reply":"2025-10-26T17:33:00.406647Z"},"papermill":{"duration":589.559816,"end_time":"2025-10-22T16:38:39.544230","exception":false,"start_time":"2025-10-22T16:28:49.984414","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Fitting 4 folds for each of 48 candidates, totalling 192 fits\nBest Cross-Validation RMSE (on log-target): 2.1466\nBest XGBoost parameters for final training:\n{'colsample_bytree': 0.7, 'max_depth': 10, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.75, 'n_estimators': 1500, 'learning_rate': 0.02, 'objective': 'reg:squarederror', 'random_state': 42, 'n_jobs': -1}\n\nSubmission file 'submission.csv' successfully created.\n","output_type":"stream"}],"execution_count":2}]}
