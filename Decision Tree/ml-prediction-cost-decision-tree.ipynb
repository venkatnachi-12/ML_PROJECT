{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116098,"databundleVersionId":14140597,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":12.77451,"end_time":"2025-10-22T16:21:29.200571","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-22T16:21:16.426061","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3f2b23bd","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-26T13:35:48.578197Z","iopub.execute_input":"2025-10-26T13:35:48.579128Z","iopub.status.idle":"2025-10-26T13:35:48.585336Z","shell.execute_reply.started":"2025-10-26T13:35:48.579100Z","shell.execute_reply":"2025-10-26T13:35:48.584306Z"},"papermill":{"duration":1.652902,"end_time":"2025-10-22T16:21:22.235391","exception":false,"start_time":"2025-10-22T16:21:20.582489","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/sample_submission.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\n","output_type":"stream"}],"execution_count":3},{"id":"6a13b3ee","cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeRegressor # The model we are now using\nfrom sklearn.metrics import mean_squared_error\n\n\nCLIP_FLOOR = 1.0        \nFREQ_THRESHOLD = 0.01   \n    \n\ntrain_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\")\n\n\ntest_ids = test_df['Hospital_Id']\ntrain_df.set_index('Hospital_Id', inplace=True)\ntest_df.set_index('Hospital_Id', inplace=True)\n\ny_train = train_df[\"Transport_Cost\"].copy()\nX_train = train_df.drop(columns=[\"Transport_Cost\"]).copy()\nX_test = test_df.copy()\n\ncombined_df = pd.concat([X_train, X_test], axis=0)\n\n# Log transformation on clipped transport cost\ny_train_transformed = np.log(y_train.clip(lower=CLIP_FLOOR))\n\n#Dropping unnecesary columns and including new columns with important information\ncombined_df['Order_Placed_Date'] = pd.to_datetime(combined_df['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Date'] = pd.to_datetime(combined_df['Delivery_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Lag_Days'] = (combined_df['Delivery_Date'] - combined_df['Order_Placed_Date']).dt.days.fillna(0).astype(int)\ncombined_df['Equipment_Volume'] = combined_df['Equipment_Height'] * combined_df['Equipment_Width']\n\n# Drop original dates and low-utility features\ncombined_df.drop(columns=['Order_Placed_Date', 'Delivery_Date', 'Supplier_Name', 'Hospital_Location'], inplace=True, errors='ignore')\n\n# Map binary columns to 0 or 1 according to binary_map\nbinary_map = {'Yes': 1, 'No': 0}\nbinary_cols_to_map = ['CrossBorder_Shipping', 'Installation_Service', 'Rural_Hospital'] \nfor col in binary_cols_to_map:\n    if col in combined_df.columns:\n        combined_df[col] = combined_df[col].map(binary_map).fillna(0)\n    \n# Add all binary columns to a list\nbinary_cols = binary_cols_to_map + ['Urgent_Shipping', 'Fragile_Equipment'] \n\n# Group Low-Frequency(rare) categories\ncategorical_cols_to_group = ['Equipment_Type', 'Transport_Method', 'Hospital_Info']\nfor col in categorical_cols_to_group:\n    if col in combined_df.columns:\n        train_counts = combined_df.iloc[:len(X_train)][col].value_counts(normalize=True)\n        low_freq_cats = train_counts[train_counts < FREQ_THRESHOLD].index\n        combined_df[col] = np.where(combined_df[col].isin(low_freq_cats), 'Other', combined_df[col])\n\n\n# Re-separate the data after preprocessing into training and testing sets\nX_train_clean = combined_df.iloc[:len(X_train)]\nX_test_clean = combined_df.iloc[len(X_train):]\n\n\n\n#Find numerical and categorical features\nnumeric_cols = [col for col in X_train_clean.select_dtypes(include=np.number).columns.tolist() if col not in binary_cols]\ncategorical_cols = X_train_clean.select_dtypes(include=['object']).columns.tolist()\n\n#Creating pipeline to do preprocessing\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ],\n    remainder='passthrough'\n)\n\n\n\n\n\n# Define base Decision Tree  model\ndt_base = DecisionTreeRegressor(\n    random_state=42\n)\n\n\ntuning_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', dt_base)\n])\n\n\nparam_grid = { 'regressor__max_depth': [5, 10, 15, 20, 25, 30], 'regressor__min_samples_split': [2, 5] }\n\n# Cross-validation initialization\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n#Hyperparameter tuning using GridSearchCV\ngrid_search = GridSearchCV(\n    tuning_pipeline, \n    param_grid, \n    cv=cv, \n    scoring='neg_root_mean_squared_error',\n    verbose=2, \n    n_jobs=-1\n)\n\n# Execute the search by fitting model on tranformed training data \ngrid_search.fit(X_train_clean, y_train_transformed)\n\n\nbest_params_found = grid_search.best_params_\nbest_score = -grid_search.best_score_\nprint(f\"Best Cross Validation RMSE on log-transformed target: {best_score:.4f}\")\n\n# Clean up parameter names for the final model\nfinal_dt_params = {k.replace('regressor__', ''): v for k, v in best_params_found.items()}\nfinal_dt_params['random_state'] = 42\n\nprint(\"Best Decision Tree parameters for final training:\",final_dt_params)\n\n\n\n# Initialize the final model\ndt_best_model = DecisionTreeRegressor(**final_dt_params)\n\n# Create the final pipeline\nfinal_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', dt_best_model)\n])\n\n# Fit the final pipeline on all training data\nfinal_pipeline.fit(X_train_clean, y_train_transformed)\n\n# Make predictions\npredictions_log = final_pipeline.predict(X_test_clean)\n\n# Inverse Transform Predictions \npredictions_original_scale = np.exp(predictions_log) \npredictions_original_scale = np.maximum(predictions_original_scale, 0)\n\n\nsubmission = pd.DataFrame({\n    'Hospital_Id': test_ids,\n    'Transport_Cost': predictions_original_scale\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-10-26T13:35:48.587081Z","iopub.execute_input":"2025-10-26T13:35:48.587381Z","iopub.status.idle":"2025-10-26T13:35:49.883050Z","shell.execute_reply.started":"2025-10-26T13:35:48.587358Z","shell.execute_reply":"2025-10-26T13:35:49.882205Z"},"papermill":{"duration":4.341526,"end_time":"2025-10-22T16:21:26.578930","exception":false,"start_time":"2025-10-22T16:21:22.237404","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 12 candidates, totalling 36 fits\nBest Cross Validation RMSE on log-transformed target: 2.1779\nBest Decision Tree parameters for final training: {'max_depth': 5, 'min_samples_split': 2, 'random_state': 42}\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.2s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.2s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.2s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.2s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.2s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.2s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=20, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=5, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=10, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.1s\n[CV] END regressor__max_depth=15, regressor__min_samples_split=5; total time=   0.2s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=25, regressor__min_samples_split=2; total time=   0.1s\n[CV] END regressor__max_depth=30, regressor__min_samples_split=2; total time=   0.1s\n","output_type":"stream"}],"execution_count":4}]}
