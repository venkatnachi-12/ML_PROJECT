{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116098,"databundleVersionId":14140597,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":13.01788,"end_time":"2025-10-25T15:18:10.082067","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-10-25T15:17:57.064187","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d4b87aae","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-10-26T17:44:45.158159Z","iopub.execute_input":"2025-10-26T17:44:45.158548Z","iopub.status.idle":"2025-10-26T17:44:45.168625Z","shell.execute_reply.started":"2025-10-26T17:44:45.158520Z","shell.execute_reply":"2025-10-26T17:44:45.167319Z"},"papermill":{"duration":1.823459,"end_time":"2025-10-25T15:18:03.413830","exception":false,"start_time":"2025-10-25T15:18:01.590371","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/sample_submission.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\n/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\n","output_type":"stream"}],"execution_count":3},{"id":"e858e7b6","cell_type":"code","source":"#Importing libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import AdaBoostRegressor  \nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# Configuration and Constants \nCLIP_FLOOR = 1.0     #Negative/zero costs clipped to this value before log transform\nFREQ_THRESHOLD = 0.05 #Group categories with frequency < 5%\nN_SPLITS_CV = 10         \nRANDOM_STATE = 42\n\n# Data Loading\ntrain_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\")\ntest_ids = test_df['Hospital_Id']\ntrain_df.set_index('Hospital_Id', inplace=True) \ntest_df.set_index('Hospital_Id', inplace=True)\n\n# Feature Engineering\ny_train = train_df[\"Transport_Cost\"].copy()\nX_train = train_df.drop(columns=[\"Transport_Cost\"]).copy()\nX_test = test_df.copy()\n\ncombined_df = pd.concat([X_train, X_test], axis=0)\n\n# Best-performing transformation: Clip and Log\ny_train_transformed = np.log(y_train.clip(lower=CLIP_FLOOR))\n\n# KEEPING THE DATE FEATURES\ncombined_df['Order_Placed_Date'] = pd.to_datetime(combined_df['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Date'] = pd.to_datetime(combined_df['Delivery_Date'], format='%m/%d/%y', errors='coerce')\ncombined_df['Delivery_Lag_Days'] = (combined_df['Delivery_Date'] - combined_df['Order_Placed_Date']).dt.days.fillna(0).astype(int)\ncombined_df['Order_Day_of_Week'] = combined_df['Order_Placed_Date'].dt.dayofweek\ncombined_df['Order_Month'] = combined_df['Order_Placed_Date'].dt.month\n\ncombined_df['Equipment_Volume'] = combined_df['Equipment_Height'] * combined_df['Equipment_Width']\ncombined_df['Equipment_Density'] = combined_df['Equipment_Weight'] / (combined_df['Equipment_Volume'] + 1e-6)\n\ncombined_df.drop(columns=['Order_Placed_Date', 'Delivery_Date', 'Supplier_Name', 'Hospital_Location'], inplace=True, errors='ignore')\n\n# Binary and Categorical Mapping\nbinary_map = {'Yes': 1, 'No': 0}\nbinary_cols = ['CrossBorder_Shipping', 'Installation_Service', 'Rural_Hospital', 'Urgent_Shipping', 'Fragile_Equipment']\n\nfor col in binary_cols:\n    if col in combined_df.columns:\n        combined_df[col] = combined_df[col].map(binary_map).fillna(0) # Fill NaNs with 0 ('No')\n\n# Group Low-Frequency Categorical Features\ncategorical_cols_to_group = ['Equipment_Type', 'Transport_Method', 'Hospital_Info']\nfor col in categorical_cols_to_group:\n    if col in combined_df.columns:\n        train_counts = combined_df.iloc[:len(X_train)][col].value_counts(normalize=True)\n        low_freq_cats = train_counts[train_counts < FREQ_THRESHOLD].index\n        combined_df[col] = np.where(combined_df[col].isin(low_freq_cats), 'Other', combined_df[col])\n\n# Final Data Preparation\nX_train_clean = combined_df.iloc[:len(X_train)]\nX_test_clean = combined_df.iloc[len(X_train):]\n\n\n# Define Preprocessing Pipeline\nnumeric_cols = [col for col in X_train_clean.select_dtypes(include=np.number).columns.tolist() if col not in binary_cols]\ncategorical_cols = X_train_clean.select_dtypes(include=['object']).columns.tolist()\n\n#Numerical columns median imputation is done \nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())  \n])\n\n#Categorical columns One-hot encoding is done\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ],\n    remainder='passthrough' # Passes through the binary columns\n)\n          \n# Fit AdaBoost Model\nada_model = AdaBoostRegressor(\n    n_estimators=300,        # Number of boosting stages\n    learning_rate=0.1,       # Shrinks contribution of each estimator\n    loss='square',           # The loss function to use when updating the weights\n    random_state=RANDOM_STATE\n)\n\n# Create the full pipeline\nadaboost_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', ada_model)\n])\n\n# Fit the pipeline on the entire training dataset\nadaboost_pipeline.fit(X_train_clean, y_train_transformed)\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import make_scorer, mean_squared_error\nimport numpy as np\n\n\nrmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n\n\ncv_strategy = KFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=RANDOM_STATE)\n\n\n\n\n\ncv_scores = cross_val_score(\n    adaboost_pipeline,\n    X_train_clean,\n    y_train_transformed,\n    cv=cv_strategy,\n    scoring='neg_root_mean_squared_error', # Use the standard string for RMSE\n    n_jobs=-1\n)\n\n\nmean_cv_rmse = -np.mean(cv_scores)\nstd_cv_rmse = np.std(cv_scores)\n\nprint(f\"Cross-Validation RMSE on log-target: {mean_cv_rmse:.4f}\")\nprint(f\"CV RMSE Standard Deviation: {std_cv_rmse:.4f}\")\n\n\nadaboost_pipeline.fit(X_train_clean, y_train_transformed)\n\n\ntest_pred_transformed = adaboost_pipeline.predict(X_test_clean)\n\n\ntest_pred = np.exp(test_pred_transformed).clip(min=1.0) # Clip at 1.0\n\n\nsubmission = pd.DataFrame({\n    'Hospital_Id': test_ids,\n    'Transport_Cost': test_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-10-26T17:44:45.170076Z","iopub.execute_input":"2025-10-26T17:44:45.170411Z","iopub.status.idle":"2025-10-26T17:45:12.929491Z","shell.execute_reply.started":"2025-10-26T17:44:45.170388Z","shell.execute_reply":"2025-10-26T17:45:12.928184Z"},"papermill":{"duration":6.045455,"end_time":"2025-10-25T15:18:09.461427","exception":false,"start_time":"2025-10-25T15:18:03.415972","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Cross-Validation RMSE on log-target: 2.7368\nCV RMSE Standard Deviation: 0.0525\nSubmission file created: submission.csv\n","output_type":"stream"}],"execution_count":4}]}
